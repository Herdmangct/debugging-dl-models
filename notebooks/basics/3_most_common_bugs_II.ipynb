{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from utils.utils import make_writer\n",
    "\n",
    "os.environ[\"TF_KERAS\"]='1'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most common bugs II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Understanding LSTM networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Batch normalization explained](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)\n",
    "- [See-RNN package](https://github.com/OverLordGoldDragon/see-rnn)\n",
    "- [Gradient clipping](http://proceedings.mlr.press/v28/pascanu13.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical instabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding and vanishing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of theory:\n",
    "\n",
    "- $X$ - input\n",
    "- $o$ - output\n",
    "- $L$ - number of layers in the network\n",
    "- $l$ - layer $l$ with the transormation $f_l$ and corresponding weights $W^l$ and the hidden variable $h^l$\n",
    "\n",
    " $$h^l = f_l(h^{l-1})$$ $$o = f_L \\circ ... \\circ f_1(X)$$\n",
    " \n",
    "If all $h^l$ and the input are vectors, one can write the gradient of $o$ with respect to any set of parameters $W^l$ as:\n",
    "\n",
    "$$\\partial_{W^l}o = \\partial_{h^{L-1}}{h^L} ... \\partial_{h^{l}}h^{l+1} \\partial_{W^{l}}h^l   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradients:\n",
    "\n",
    "- Historically sigmoid was used as an activation function.\n",
    "- It resembles a thresholding function and was appealing, since neural nets were inspired by biological neural networks, where biological neurons either fire fully or not at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = tf.Variable(tf.range(-8.0, 8.0, 0.1))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.nn.leaky_relu(x)\n",
    "\n",
    "grad = tape.gradient(y, x).numpy()\n",
    "x = x.numpy()\n",
    "y = y.numpy()\n",
    "    \n",
    "plt.plot(x, y, 'b', label='sigmoid')\n",
    "plt.plot(x, grad, 'r', label='gradient') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradients vanish for both big and small inputs.\n",
    "- Multiply many layers and if the inputs are not near zero, then a gradient can vanish.\n",
    "- Gradient cut off at some layer --> difficult to build deep nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "- What's the standard solution for this specific instance of the problem?\n",
    "- Make a similar plot for the solution.\n",
    "- What is the problem of the standard solution? Can we improve it? Please, plot it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradients in RNNs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See-rnn package helps debug and analyze RNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_gradients' from 'see_rnn' (/Users/grahamherdman/opt/anaconda3/lib/python3.8/site-packages/see_rnn/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-17292950f47e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msee_rnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_0D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mipt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mrnn_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_gradients' from 'see_rnn' (/Users/grahamherdman/opt/anaconda3/lib/python3.8/site-packages/see_rnn/__init__.py)"
     ]
    }
   ],
   "source": [
    "from see_rnn import get_gradients, features_0D, features_1D, features_2D\n",
    "\n",
    "def make_model(rnn_layer, batch_shape, units):\n",
    "    ipt = tf.keras.layers.Input(batch_shape=batch_shape)\n",
    "    x   = rnn_layer(units, activation='tanh', return_sequences=True)(ipt)\n",
    "    out = rnn_layer(units, activation='tanh', return_sequences=False)(x)\n",
    "    model = tf.keras.models.Model(ipt, out)\n",
    "    model.compile(tf.keras.optimizers.Adam(4e-3), 'mse')\n",
    "    return model\n",
    "    \n",
    "def make_data(batch_shape):\n",
    "    return np.random.randn(*batch_shape), \\\n",
    "           np.random.uniform(-1, 1, (batch_shape[0], units))\n",
    "\n",
    "def train_model(model, iterations, batch_shape):\n",
    "    x, y = make_data(batch_shape)\n",
    "    for i in range(iterations):\n",
    "        model.train_on_batch(x, y)\n",
    "        print(end='.')  # progbar\n",
    "        if i % 40 == 0:\n",
    "            x, y = make_data(batch_shape)\n",
    "\n",
    "units = 16\n",
    "batch_shape = (32, 100, 2*units)\n",
    "\n",
    "model_rnn = make_model(tf.keras.layers.SimpleRNN, batch_shape, units)\n",
    "model_lstm = make_model(tf.keras.layers.LSTM, batch_shape, units)\n",
    "train_model(model_rnn, 300, batch_shape)\n",
    "train_model(model_lstm, 300, batch_shape)\n",
    "\n",
    "x, y  = make_data(batch_shape)\n",
    "grads_all_rnn  = get_gradients(model_rnn, 1, x, y)\n",
    "grads_all_lstm  = get_gradients(model_lstm, 1, x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_1D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1ec53e6d3e91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot RNN VS LSTM 1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeatures_1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_all_rnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_xy_ticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfeatures_1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_all_lstm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_xy_ticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_1D' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot RNN VS LSTM 1D\n",
    "features_1D(grads_all_rnn[:, :, :5], n_rows=5, show_xy_ticks=[1,1])\n",
    "features_1D(grads_all_lstm[:, :, :5], n_rows=5, show_xy_ticks=[1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploding gradients and gradient clipping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single matrix \n",
      " \n",
      " [[ 1.4718187  -0.4729736  -1.0950321   0.3213188 ]\n",
      " [-0.09319968  0.05123096  1.7287505  -1.5272131 ]\n",
      " [-0.82670426 -2.5330334  -1.4010104   0.42914835]\n",
      " [-0.9341722   0.27875     1.983981    0.31062376]]\n",
      "\n",
      "After multiplying 100 matrices \n",
      " \n",
      " [[ 9.6846386e+23 -1.5954567e+24  7.0441775e+23  1.5792985e+24]\n",
      " [-3.0949904e+24  5.0987175e+24 -2.2511592e+24 -5.0470793e+24]\n",
      " [ 4.9597640e+24 -8.1707634e+24  3.6075124e+24  8.0880118e+24]\n",
      " [-4.4286547e+23  7.2958083e+23 -3.2212076e+23 -7.2219190e+23]]\n"
     ]
    }
   ],
   "source": [
    "M = tf.random.normal((4, 4))\n",
    "print(f'A single matrix \\n \\n {M.numpy()}')\n",
    "for i in range(100):\n",
    "    M = tf.matmul(M, tf.random.normal((4, 4)))\n",
    "\n",
    "print(f'\\nAfter multiplying 100 matrices \\n \\n {M.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient clipping\n",
    "\n",
    "- Clip a gradient by norm:\n",
    "    - $ \\textbf{g} \\gets min\\left(1, \\frac{\\theta}{||\\textbf{g}||}\\textbf{g} \\right)$\n",
    "    - For example: $$\\textbf{g}= [-2, 3, 6]$$ $$\\theta = 5$$ $$||\\textbf{g}|| = 7$$ $$\\textbf{g} \\gets [-2, 3, 6]\\cdot \\frac{5}{7}$$\n",
    "    \n",
    "- Clip gradient by value:\n",
    "    - If $g_i < \\theta_1$, then $g_i \\gets \\theta_1$ and $g_i > \\theta_2$, then $g_i \\gets \\theta_2$\n",
    "    - For example: $$\\textbf{g}= [-2, 3, 10]$$ $$\\theta_1 = 0, \\theta_2 = 5$$  $$ \\textbf{g} \\gets [0, 3, 5]$$\n",
    "\n",
    "    \n",
    "- Clip gradient by global norm:\n",
    "    - Rescales a list of tensors so that the total norm of the vector of all their norms does not exceed a threshold.\n",
    "    - For example: $$\\textbf{g}_1 = [-2, 3, 6]$$ $$\\textbf{g}_2= [-4, 6, 12]$$ $$\\theta = 14$$ $$||\\textbf{g}_1|| = 7$$ $$||\\textbf{g}_2|| = 14$$ $$\\textbf{g}_1 \\gets [-2, 3, 6]\\cdot \\frac{14}{\\sqrt{7^2 + 14^2}}$$ $$\\textbf{g}_2 \\gets [-4, 6, 12]\\cdot \\frac{14}{\\sqrt{7^2 + 14^2}} $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :].astype('float32'), X[n_train:, :].astype('float32')\n",
    "trainy, testy = y[:n_train].astype('float32'), y[n_train:].astype('float32')\n",
    "\n",
    "# Creat tf.Datasets \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((trainX, trainy)).shuffle(trainX.shape[0]).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((testX, testy)).shuffle(testX.shape[0]).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressorNet(tf.keras.Model):\n",
    "    def __init__(self, input_shape, optimizer):\n",
    "        super(RegressorNet, self).__init__()\n",
    "    \n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError()\n",
    "        self.optimizer = optimizer\n",
    "        self.regressor = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(input_shape),\n",
    "            tf.keras.layers.Dense(25, activation='relu', kernel_initializer='he_uniform', name='dense_1'),\n",
    "            tf.keras.layers.Dense(1, activation='linear', name='out')\n",
    "        ])\n",
    "    \n",
    "    def summary(self):\n",
    "        self.regressor.summary()\n",
    "    \n",
    "    def call(self, X):\n",
    "        return self.regressor(X)\n",
    "    \n",
    "    def get_loss(self, X, y_true):\n",
    "        y_pred = self(X)\n",
    "        l2_loss = self.loss_object(y_true, y_pred)\n",
    "        return l2_loss\n",
    "    \n",
    "    def grad_step(self, X, y_true):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.get_loss(X, y_true)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 25)                525       \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 551\n",
      "Trainable params: 551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Specify an optimizer and an instance of the model\n",
    "optimizer = tf.keras.optimizers.SGD(0.01, 0.9)\n",
    "model_non_clipped = RegressorNet(input_shape=trainX.shape[1], optimizer=optimizer)\n",
    "# Show summary\n",
    "model_non_clipped.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss objects for calculation of the mean loss across batches\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_dataset, test_dataset, save_dir):\n",
    "    \n",
    "    writer = make_writer(os.path.join('summaries'), save_dir)\n",
    "    \n",
    "    for epoch in range(0, epochs + 1):\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        test_loss.reset_states()\n",
    "\n",
    "    \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch {} is running...'.format(epoch))\n",
    "        \n",
    "        for X, y in train_dataset:\n",
    "            # Gradient update step\n",
    "            loss_train, gradients = model.grad_step(X, y)\n",
    "            train_loss(loss_train)\n",
    "            \n",
    "        for X, y in test_dataset:\n",
    "            # Gradient update step\n",
    "            loss_test = model.get_loss(X, y)\n",
    "            test_loss(loss_test)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Train loss: {train_loss.result()}')\n",
    "\n",
    "        # Tensorboard\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('Test loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('Train loss', train_loss.result(), step=epoch)\n",
    "            \n",
    "            for layer_number, layer in enumerate(model.trainable_variables):\n",
    "                tf.summary.histogram(layer.name, gradients[layer_number], step=epoch, buckets=1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a tensorboard directory: summaries/exploding_grads/no_clipping/\n",
      "Epoch 0 is running...\n",
      "Train loss: 18787.44140625\n",
      "Epoch 10 is running...\n",
      "Train loss: 54.26221466064453\n",
      "Epoch 20 is running...\n",
      "Train loss: 40.599632263183594\n",
      "Epoch 30 is running...\n",
      "Train loss: 23.710163116455078\n",
      "Epoch 40 is running...\n",
      "Train loss: 29.381074905395508\n",
      "Epoch 50 is running...\n",
      "Train loss: 24.959970474243164\n",
      "Epoch 60 is running...\n",
      "Train loss: 13.975386619567871\n",
      "Epoch 70 is running...\n",
      "Train loss: 19.336666107177734\n",
      "Epoch 80 is running...\n",
      "Train loss: 10.304941177368164\n",
      "Epoch 90 is running...\n",
      "Train loss: 13.372834205627441\n",
      "Epoch 100 is running...\n",
      "Train loss: 12.569353103637695\n"
     ]
    }
   ],
   "source": [
    "train(model_non_clipped, 100, train_dataset, test_dataset, 'exploding_grads/no_clipping/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Implement gradient clipping by norm OR by value OR by global norm in a new class RegressorNetClipped.\n",
    "- Plot gradients with clipping.\n",
    "\n",
    "A threshold is a parameter. In most of the cases it's a small number, usually around 1.\n",
    "However, one can experiment with that, bigger numbers speed up learning, but too big of a threshold can make it unstable.\n",
    "Another rule of thumb to choose a threshold is to monitor an average norm of the gradients for a big number of updates, then set the threshold to 5-10 times the value of that average.\n",
    "\n",
    "Hint: check `tf.clip_by_value`, `tf.clip_by_norm`, `tf.clip_by_global_norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressorNetClipped(tf.keras.Model):\n",
    "    def __init__(self, input_shape, optimizer):\n",
    "        super(RegressorNetClipped, self).__init__()\n",
    "    \n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError()\n",
    "        self.optimizer = optimizer\n",
    "        self.regressor = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(input_shape),\n",
    "            tf.keras.layers.Dense(25, activation='relu', kernel_initializer='he_uniform', name='dense_1'),\n",
    "            tf.keras.layers.Dense(1, activation='linear', name='out')\n",
    "        ])\n",
    "    \n",
    "    def summary(self):\n",
    "        self.regressor.summary()\n",
    "    \n",
    "    def call(self, X):\n",
    "        return self.regressor(X)\n",
    "    \n",
    "    def get_loss(self, X, y_true):\n",
    "        y_pred = self(X)\n",
    "        l2_loss = self.loss_object(y_true, y_pred)\n",
    "        return l2_loss\n",
    "    \n",
    "    def grad_step(self, X, y_true):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.get_loss(X, y_true)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        gradients = [tf.clip_by_value(g, -1, 1) for g in gradients]\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 25)                525       \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 551\n",
      "Trainable params: 551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Specify an optimizer and an instance of the model\n",
    "optimizer = tf.keras.optimizers.SGD(0.01, 0.9)\n",
    "clipped_model = RegressorNetClipped(input_shape=trainX.shape[1], optimizer=optimizer)\n",
    "# Show summary\n",
    "clipped_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a tensorboard directory: summaries/exploding_grads/no_clipping/\n",
      "Epoch 0 is running...\n",
      "Train loss: 19057.73046875\n",
      "Epoch 10 is running...\n",
      "Train loss: 86.17288208007812\n",
      "Epoch 20 is running...\n",
      "Train loss: 30.512033462524414\n",
      "Epoch 30 is running...\n",
      "Train loss: 13.195301055908203\n",
      "Epoch 40 is running...\n",
      "Train loss: 25.67711067199707\n",
      "Epoch 50 is running...\n",
      "Train loss: 18.025390625\n",
      "Epoch 60 is running...\n",
      "Train loss: 17.931123733520508\n",
      "Epoch 70 is running...\n",
      "Train loss: 16.180973052978516\n",
      "Epoch 80 is running...\n",
      "Train loss: 16.538421630859375\n",
      "Epoch 90 is running...\n",
      "Train loss: 16.898441314697266\n",
      "Epoch 100 is running...\n",
      "Train loss: 5.4275288581848145\n"
     ]
    }
   ],
   "source": [
    "train(clipped_model, 100, train_dataset, test_dataset, 'exploding_grads/no_clipping/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOM errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common issues and causes\n",
    "\n",
    "- Too big a tensor:\n",
    "    - Too large a batch size for your model \n",
    "    - Too many fully connected layers\n",
    "- Too much data:\n",
    "    - Loading a too big dataset into memory instead of using, e.g. tf.data queue loading\n",
    "    - Allocating to large a buffer for dataset creation\n",
    "- Duplicating operations:\n",
    "    - Memory leak due to creating multiple models at the same time\n",
    "    - Repeatedly creating an operation (e.g. in a function that gets called many times)\n",
    "- Other processes:\n",
    "    - Other processes taking GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of you will work with images. Here is an easy way to load images off disk batch by batch, so you won't run out of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use tf.keras.preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "```\n",
    "\n",
    "- This code assumes that the images are stored in a directory with sub-directories for each label.\n",
    "- The output is tf.data.Dataset object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
