{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "from ig_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated gradients use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_v1 = tf.keras.Sequential([\n",
    "    hub.KerasLayer(name='inception_v1', \n",
    "                   handle=\"https://tfhub.dev/google/imagenet/inception_v1/classification/4\", \n",
    "                   trainable=False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_labels = load_imagenet_labels('https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and plot a few new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_name_url {image_name: origin_url}\n",
    "img_name_url = {\n",
    "    'Golden Retriever': 'https://storage.googleapis.com/applied-dl/temp/Golden_retriever.jpg',\n",
    "    'Yellow Labrador Retriever': 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg',\n",
    "    'Military Uniform (Grace Hopper)': 'https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg',\n",
    "    'Military Uniform (General Ulysses S. Grant)': 'https://storage.googleapis.com/applied-dl/temp/General_Ulysses_S._Grant%2C_Union_Army_(6186252896).jpg',\n",
    "    'Military Uniform (Greek Presidential Guard)': 'https://storage.googleapis.com/applied-dl/temp/Greek_guard_uniforms_1.jpg',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_name_path {image_name: downloaded_image_local_path}\n",
    "img_name_path = {name: tf.keras.utils.get_file(name, url) for (name, url) in img_name_url.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_name_tensors {image_name: read_image_tensor}\n",
    "img_name_tensors = {name: read_image(img_path) for (name, img_path) in img_name_path.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, img_tensor in img_name_tensors.items():\n",
    "    plot_img_predictions(model=inception_v1, img_tensor=img_tensor, name=name, labels=imagenet_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_baseline_tensors = {\n",
    "    'Baseline Image: Black': tf.zeros(shape=(224, 224, 3)),\n",
    "    'Baseline Image: White': tf.ones(shape=(224, 224, 3)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case: understanding feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the difference between a Golden Retriever and Labrador Retriever?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_img_predictions(\n",
    "    model=inception_v1, \n",
    "    img_tensor=img_name_tensors['Golden Retriever'], \n",
    "    name='Golden Retiever', \n",
    "    labels=imagenet_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_img_predictions(\n",
    "    model=inception_v1, \n",
    "    img_tensor=img_name_tensors['Yellow Labrador Retriever'], \n",
    "    name='Yellow Labrador Retiever', \n",
    "    labels=imagenet_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model is quite confident about the Golden Retriever in the top image.\n",
    "- In comparison, the model is relatively less confident about its correct prediction of the Labrador Retriever in the second image and also sees some shades of similarity with the Golden Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_img_attributions(model=inception_v1,\n",
    "                          image=img_name_tensors['Golden Retriever'],\n",
    "                          baseline=name_baseline_tensors['Baseline Image: Black'],                       \n",
    "                          target_class_idx=tf.constant(208),\n",
    "                          m_steps=tf.constant(1250),\n",
    "                          cmap=None,\n",
    "                          overlay_alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_img_attributions(model=inception_v1,\n",
    "                          image=img_name_tensors['Yellow Labrador Retriever'],\n",
    "                          baseline=name_baseline_tensors['Baseline Image: Black'],                          \n",
    "                          target_class_idx=tf.constant(208),\n",
    "                          m_steps=tf.constant(1250),\n",
    "                          cmap=None,\n",
    "                          overlay_alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model uses different features to identify both breeds.\n",
    "- We can use this to make our model better, for example, by retraining with more pictures of each breed and applying augmentations to the textures and colours of the dogs' coats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case: debugging data skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training-serving data skew, a difference between performance during training and during model serving.\n",
    "- Tracking IG feature importances across time (e.g. \"next day\" splits) and data splits (e.g. train/val/test splits) allows for meaningful monitoring of train-serving feature drift and skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider an example with military uniforms that change over time and also geographical location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "military_uniforms = ['Military Uniform (Grace Hopper)',\n",
    "                     'Military Uniform (General Ulysses S. Grant)',\n",
    "                     'Military Uniform (Greek Presidential Guard)'] \n",
    "\n",
    "for name in military_uniforms:\n",
    "    plot_img_predictions(inception_v1, img_name_tensors[name], name, imagenet_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot IG for all images and visually inspect them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_img_attributions(model=inception_v1,\n",
    "                          image=img_name_tensors['Military Uniform (Grace Hopper)'],\n",
    "                          baseline=name_baseline_tensors['Baseline Image: Black'],                          \n",
    "                          target_class_idx=tf.constant(653),\n",
    "                          m_steps=tf.constant(2000),\n",
    "                          cmap=None,\n",
    "                          overlay_alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Grace Hopper](https://en.wikipedia.org/wiki/Grace_Hopper)**\n",
    "\n",
    "- The model correctly classifies the first image of United States Rear Admiral and Computer Scientist, Grace Hopper, under the class \"military uniform\" above. \n",
    "- You can see that brightest intensity pixels are focused around the shirt colar and tie, military insignia on the jacket and hat, and various pixel areas around her face. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_img_attributions(model=inception_v1,\n",
    "                          image=img_name_tensors['Military Uniform (General Ulysses S. Grant)'],\n",
    "                          baseline=name_baseline_tensors['Baseline Image: White'],                          \n",
    "                          target_class_idx=tf.constant(870),\n",
    "                          m_steps=tf.constant(1500),\n",
    "                          cmap=None,\n",
    "                          overlay_alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Ulysses S. Grant](https://en.wikipedia.org/wiki/Ulysses_S._Grant)**\n",
    "\n",
    "- The model predictions above, you can see not very well as the model incorrectly predicts a trench coat and suit above a military uniform, since the uniform is old.\n",
    "- Since this is a faded black and white image with prominent darker features, a white baseline is a better choice.\n",
    "- The model identified the military insignia patch on the right shoulder, face, collar, jacket buttons, and pixels around the edges of the coat. \n",
    "- We can improve model performance by adding data augmentation to the input data pipeline to include additional colourless images and image translations as well as additional example images with military coats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_img_attributions(model=inception_v1,\n",
    "                          image=img_name_tensors['Military Uniform (Greek Presidential Guard)'],\n",
    "                          baseline=name_baseline_tensors['Baseline Image: Black'],                          \n",
    "                          target_class_idx=tf.constant(653),\n",
    "                          m_steps=tf.constant(1500),\n",
    "                          cmap=None,\n",
    "                          overlay_alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Greek Presidential Guard](https://en.wikipedia.org/wiki/Presidential_Guard_(Greece))**\n",
    "\n",
    "- The model incorrectly predicted the image of a Greek Presidential Guard as a vestment with low confidence. \n",
    "- There is not enough examples of such uniform in the training set.\n",
    "- Re-training the model on this more diverse sampling of the input space of Greece military uniforms, in particular those that emphasize military insignia, as well as utilizing weighting strategies during training can help with biased data and further improve model performance and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case: debugging model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using IG feature attributions for debugging, you are looking for insights into the following questions:\n",
    "\n",
    "*   Which features are important? \n",
    "*   How well does a model's learnt features generalize? \n",
    "*   Does a model learn \"incorrect\" or spurious features in the image beyond the true class object?\n",
    "*   What features did my model miss?\n",
    "*   Comparing correct and incorrect examples of the same class, what is the difference in the feature attributions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aapply 3 transformations to the \"Yellow Labrador Retriever\" image and constrast correct and incorrect IG feature attributions to gain insight into the model's limitations.**\n",
    "- Transformations:\n",
    "    - rotate the image by 90 degrees\n",
    "    - flip upside down\n",
    "    - zoom in range (0.45, 0.45)\n",
    "- Plot images with class predictions.\n",
    "- *Separately* calculate IG, using `integrated_gradients` with black baseline and number of steps 1250.\n",
    "- Plot IG overlayed over a zoomed image, over an original image and one plot with both attribution masks overlayed. Use different `cmap`s for both masks.\n",
    "- What conclusions about CNNs can you draw from these results?\n",
    "- What can be done to improve learning?\n",
    "\n",
    "Hint: use `tf.image.rot90`, `tf.image.flip_up_down`, `tf.keras.prepocessing.image.random_zoom` with default settings and the specified above zoom and `tf.reduce_sum(tf.abs(attributions), axis=-1)` for creating an attribution mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We concentrated on the concrete use cases for applying IG.\n",
    "- Disccussed potential problems stemming either from data or model training.\n",
    "- Look at recommendations for further model improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
