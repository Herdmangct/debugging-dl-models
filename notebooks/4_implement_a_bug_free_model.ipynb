{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a bug free model\n",
    "\n",
    "- Key idea: **Be suspicious and start simple** \n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [Debug a deep learning network](https://medium.com/@jonathan_hui/debug-a-deep-learning-network-part-5-1123c20f960d)\n",
    "- [Troubleshooting deep learning models](https://www.youtube.com/watch?v=GwGTwPcG0YM&feature=youtu.be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy for debugging in pseudocode:\n",
    "\n",
    "1. Start simple\n",
    "2. Implement and debug\n",
    "3. Evaluate:\n",
    "    - if meets requirements --> Done\n",
    "    - else:\n",
    "        - Improve model/data and go to step 2.\n",
    "        - OR tune hyperparameters and go to step 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- Choose the simplest model and data possible\n",
    "- Once it runs, overfit a single batch and reproduce a known result. If a result is not known, then try to come up with some realistic baseline, using either common sense or human level performance.\n",
    "- Apply bias-variance trade off\n",
    "- Tune hyperparameters\n",
    "- Bigger model if you underfit, add data or regularize if you overfitexampleexample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start simple \n",
    "\n",
    "### 1.1 Choose a simple architecture\n",
    "\n",
    "|  | Start |  Later |\n",
    "| ----------- | ----------- | ---------- |\n",
    "| Images | LeNet-like architecture  | ResNet, Inception |\n",
    "| Sequences | LSTM with one hidden layer or temporal convolutions | Attention model |\n",
    "| Other | Fully connected with one hidden layer | Depends on the problem |\n",
    "\n",
    "If there are different input modalities, use an appropriate architecture for each and then mix with a fully connected layer.example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Defaults to start with\n",
    "\n",
    "- Optimizer: Adam with the magical LR 3e-4\n",
    "- Activations: ReLU for FC and Conv, tanh for LSTMs\n",
    "- Initialization: Glorot Normal, Glorot Uniform (simple defaults used for the layers in TF2)\n",
    "- No regularization\n",
    "- No batch, layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF2 defaults\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "initializer = tf.keras.initializers.GlorotUniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Normalize input data\n",
    "\n",
    "#### Min-max\n",
    "\n",
    "Scales to [0, 1]\n",
    "- doesn't shift/center the data\n",
    "- retains sparsity\n",
    "- retains zero values\n",
    "    \n",
    "\n",
    "$$ \\hat{x} = \\frac{x - x_{min}}{x_{max} - x_{min}} $$\n",
    "\n",
    "- use when you do not know the distribution of your data\n",
    "- when you know the distribution is not Gaussian\n",
    "- algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks\n",
    "- sensitive to outliers\n",
    "\n",
    "#### Max-abs \n",
    "\n",
    "Scales to `[-1, 1]`\n",
    "- divide by largest maximum value\n",
    "\n",
    "#### Images\n",
    "\n",
    "- TF2: divide by 255\n",
    "- PyTorch: divide by 255 and then:\n",
    "\n",
    "$$ \\hat{x} = \\frac{x - \\mu }{\\sigma} $$\n",
    "$$ \\hat{x} = \\frac{\\hat{x} - 0.5}{0.5}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Simplify the problem\n",
    "\n",
    "It often makes sense to do as a starting point\n",
    "\n",
    "- For example, reduce the training set size\n",
    "- Use a smaller number of classes, image size, etc.\n",
    "- Create a synthetic training set that is easier to work with\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Implement and debug\n",
    "\n",
    "- Get your model to run\n",
    "- Overfit a single batch\n",
    "- Compare to a known result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 General advice for implementing your model\n",
    "\n",
    "- Minimum number of line of codes for your version 1 (rule of thumb < 200 lines, not counting already tested components)\n",
    "- Use of the shell components:\n",
    "    - Keras for simpler tasks where no to little changes for default behaviour of functions is needed\n",
    "    - Pure TF2, but with tf.keras.layers, tf.losses, etc. when more flexibility is needed\n",
    "- Start with a dataset that loads into memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Overfit a single batch\n",
    "\n",
    "Assuming your model runs, try to overfit a single batch:\n",
    "\n",
    "- Error goes up:\n",
    "    - Flipped the sign of the loss/gradient\n",
    "    - LR too high\n",
    "    - Softmax taken over wrong dimension \n",
    "- Error explodes:\n",
    "    - Numerical issue. Check all exp, log, div operations, clip gradients\n",
    "    - LR too high \n",
    "- Error oscillates:\n",
    "    - Data labels corrupted\n",
    "    - LR too high\n",
    "- Error plateaus:\n",
    "    - LR too low\n",
    "    - Gradients not flowing through the whole model\n",
    "    - Too much regularization\n",
    "    - Incorrect input to loss function (e.g. softmax instead of logits, ReLU on output)\n",
    "    - Data labels corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Compare to a known result\n",
    "\n",
    "| Usefulness in decreasing order | Source |\n",
    "| ----------- | ----------- |\n",
    "| 1 | Oficial model implementation evaluated on similar dataset |\n",
    "| 2 | Oficial model implementation evaluated on a benchmark (e.g. MNIST) |\n",
    "| 3 | Unoficial model implementation |\n",
    "| 4 | Results from a paper (with no code |\n",
    "| 5 | Results from your model on a benchmark dataset (e.g. MNIST) |\n",
    "| 6 | Compare similar model on a similar dataset |\n",
    "| 7 | Super simple baseline (avg. of all outputs, linear regression, common sense) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
